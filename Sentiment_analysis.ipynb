{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from os import getcwd\n",
    "from nltk.corpus import twitter_samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Paulius\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and cleaning data\n",
    "Here I deleted neutral posts and changed category's values so that it would be easier for sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pos = all_positive_tweets[4000:]\n",
    "train_pos = all_positive_tweets[:4000]\n",
    "test_neg = all_negative_tweets[4000:]\n",
    "train_neg = all_negative_tweets[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@BhaktisBanter @PallaviRuhail This one is irresistible :)\\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (2000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data\n",
    "First we decapitalize all words and leave only proper words. Then we turn 5000 words into numeric values and pad texts so they are all the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [i.lower() for i in train_x]\n",
    "train_x = [re.sub('[^a-zA-z0-9\\s]','',x) for x in train_x]\n",
    "test_x = [i.lower() for i in test_x]\n",
    "test_x = [re.sub('[^a-zA-z0-9\\s]','',x) for x in test_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bhaktisbanter pallaviruhail this one is irresistible \\nflipkartfashionfriday httptcoebz0l2venm'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000, split=' ')\n",
    "tokenizer.fit_on_texts(train_x)\n",
    "tokenizer.fit_on_texts(test_x)\n",
    "train_x = tokenizer.texts_to_sequences(train_x)\n",
    "train_x = pad_sequences(train_x, maxlen = 256)\n",
    "test_x = tokenizer.texts_to_sequences(test_x)\n",
    "test_x = pad_sequences(test_x, maxlen = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,  632, 1114,   19,   57,\n",
       "         12, 3194,  633])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Tokenizer, because we will need it in web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./models/tokenizer.pickle\", \"wb\") as tok:\n",
    "    pickle.dump(tokenizer, tok, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(8000, 256, input_length = train_x.shape[1]),\n",
    "    keras.layers.SpatialDropout1D(0.2),\n",
    "    keras.layers.LSTM(128, dropout = 0.2, recurrent_dropout = 0.2),\n",
    "    keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "168/168 [==============================] - 137s 801ms/step - loss: 0.5479 - accuracy: 0.7479 - val_loss: 1.2085 - val_accuracy: 0.2779\n",
      "Epoch 2/15\n",
      "168/168 [==============================] - 133s 791ms/step - loss: 0.3144 - accuracy: 0.8689 - val_loss: 1.0015 - val_accuracy: 0.5013\n",
      "Epoch 3/15\n",
      "168/168 [==============================] - 132s 786ms/step - loss: 0.2146 - accuracy: 0.9161 - val_loss: 1.0671 - val_accuracy: 0.6153\n",
      "Epoch 4/15\n",
      "168/168 [==============================] - 130s 776ms/step - loss: 0.1536 - accuracy: 0.9403 - val_loss: 1.4124 - val_accuracy: 0.5377\n",
      "Epoch 5/15\n",
      "168/168 [==============================] - 131s 782ms/step - loss: 0.1041 - accuracy: 0.9598 - val_loss: 1.9269 - val_accuracy: 0.4903\n",
      "Epoch 6/15\n",
      "168/168 [==============================] - 133s 790ms/step - loss: 0.0849 - accuracy: 0.9652 - val_loss: 2.2420 - val_accuracy: 0.4548\n",
      "Epoch 7/15\n",
      "168/168 [==============================] - 131s 778ms/step - loss: 0.0785 - accuracy: 0.9667 - val_loss: 2.0208 - val_accuracy: 0.5256\n",
      "Epoch 8/15\n",
      "168/168 [==============================] - 131s 778ms/step - loss: 0.0616 - accuracy: 0.9709 - val_loss: 2.2706 - val_accuracy: 0.5638\n",
      "Epoch 9/15\n",
      "168/168 [==============================] - 131s 781ms/step - loss: 0.0510 - accuracy: 0.9754 - val_loss: 2.6238 - val_accuracy: 0.4960\n",
      "Epoch 10/15\n",
      "168/168 [==============================] - 134s 798ms/step - loss: 0.0527 - accuracy: 0.9762 - val_loss: 2.5386 - val_accuracy: 0.4710\n",
      "Epoch 11/15\n",
      "168/168 [==============================] - 135s 805ms/step - loss: 0.0443 - accuracy: 0.9767 - val_loss: 2.7535 - val_accuracy: 0.5100\n",
      "Epoch 12/15\n",
      "168/168 [==============================] - 137s 818ms/step - loss: 0.0326 - accuracy: 0.9839 - val_loss: 2.8921 - val_accuracy: 0.5123\n",
      "Epoch 13/15\n",
      "168/168 [==============================] - 151s 902ms/step - loss: 0.0327 - accuracy: 0.9837 - val_loss: 3.6519 - val_accuracy: 0.5157\n",
      "Epoch 14/15\n",
      "168/168 [==============================] - 199s 1s/step - loss: 0.0393 - accuracy: 0.9816 - val_loss: 2.8545 - val_accuracy: 0.4922\n",
      "Epoch 15/15\n",
      "126/168 [=====================>........] - ETA: 45s - loss: 0.0440 - accuracy: 0.9844"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer = \"adam\", metrics=['accuracy'])\n",
    "model.fit(train_x, train_y, epochs = 15, validation_split = 0.33, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = [\"I hate this nonsense film\"]\n",
    "post = tokenizer.texts_to_sequences(post)\n",
    "post = pad_sequences(post, maxlen=28, dtype='int32', value=0)\n",
    "print(post)\n",
    "prediction = model.predict(post)\n",
    "round(float(prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./models/Model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
